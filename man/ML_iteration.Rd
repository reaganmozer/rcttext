% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ml_iteration.R
\name{ML_iteration}
\alias{ML_iteration}
\alias{ML_iterations}
\title{Iterative Machine Learning with Performance Tracking}
\usage{
ML_iteration(
  x,
  y,
  n_iteration = 1,
  training_portion = 0.8,
  trCon = NULL,
  preProc = NULL,
  n.tune = NULL,
  model = NULL,
  grid = NULL,
  outcome = NULL
)

ML_iterations(
  x,
  y,
  n_iteration = 1,
  training_portions = 0.8,
  trCon = NULL,
  preProc = NULL,
  n.tune = NULL,
  model = NULL,
  grid = NULL,
  outcome = NULL
)
}
\arguments{
\item{x}{Matrix or data frame of predictor variables.}

\item{y}{Vector of outcome variable (continuous or categorical).}

\item{n_iteration}{Number of times to repeat model training and evaluation
for each training set size (defaults to 1).}

\item{training_portion}{Proportion of data used for training (defaults to 0.8).}

\item{trCon}{Train control object for caret (optional).}

\item{preProc}{Preprocessing methods for caret (optional).}

\item{n.tune}{Number of tuning parameter combinations (optional).}

\item{model}{Machine learning model to use (e.g., "rf", "xgbTree").}

\item{grid}{Tuning grid for hyperparameter search (optional).}

\item{outcome}{Type of outcome variable: "continuous" or "categorical".}

\item{training_portions}{A vector of proportions (between 0 and 1)
indicating the fraction of data to be used for training in each iteration.}
}
\value{
A list containing results for each iteration, including:
  * The trained model (`ML_model`)
  * Predicted values on the test set (`predicted_values`)
  * Values of predictor variables on the test set (`test_values`)
  * Performance metrics (RMSE, MAE, correlation, R-squared for continuous;
    confusion matrix, accuracy, kappa values for categorical)
  * Execution time for each iteration (`execution_time`)

A nested list, where the first level corresponds
to different training set sizes and the second level contains results
for each iteration within that training set size.
Each iteration's results include:
  * The trained model (`ML_model`)
  * Predicted values on the test set (`predicted_values`)
  * Values of predictor variables on the test set (`test_values`)
  * Performance metrics (RMSE, MAE, correlation, R-squared for continuous;
    confusion matrix, accuracy, kappa values for categorical)
  * Execution time for each iteration (`execution_time`)
}
\description{
Trains a machine learning model multiple times, splitting the data into
training and testing sets for each iteration. Evaluates and stores
performance metrics for each run, allowing for assessment of model
stability and generalization.

Trains and evaluates a specified machine learning model iteratively,
using different proportions of the dataset for training in each iteration.
This function facilitates the exploration of how model performance
varies with changes in the amount of training data. Hyperparameter
tuning can be optionally incorporated to optimize the model within each
iteration.
}
\details{
This function provides a standardized way to train and evaluate models
within a larger analysis, facilitating comparison across different models,
hyperparameters, or repeated runs. It handles both regression (continuous outcome)
and classification (categorical outcome) problems, providing appropriate evaluation
metrics for each.

This function helps you understand how your chosen model performs with
varying amounts of training data, which is crucial for assessing its
potential in real-world scenarios with limited data. The inclusion of
hyperparameter tuning can further optimize the model's performance
for each training set size.
}
\examples{

# Load texts
data("toy_reads")

# Generate text features
feats = generate_features( toy_reads$text, meta=toy_reads,
                           sent = TRUE,
                           clean_features = TRUE,
                           read = c("Flesch","Flesch.Kincaid", "ARI"),
                           ld=c("TTR","R","K"),
                           ignore=c("ID"),
                           verbose = TRUE )

# Preprocess the feature space to remove collinear features
# and features with near-zero variance
X_all = dplyr::select( feats,
                       -ID, -Q1, -Q2, -text, -more )
X_all = predict(caret::preProcess( X_all, method = c("nzv","corr"),
                                   uniqueCut=2, cutoff=0.95), X_all )
caret::findLinearCombos(X_all) # sanity check to make sure no redundant features

# Transform all variables as numeric variables
X_all[] <- lapply(X_all,
                  function(x) if(is.character(x)) as.numeric(as.factor(x)) else x)

# Extract outcome variables
all_Scores <- toy_reads$Q1

## Set parameters
X <- X_all
Y <- all_Scores
n_iter <- 2
n_tune <- 2
control <- caret::trainControl(method = 'cv')
preProc <- 'zv'
outcome <- 'continuous'
portion <- 0.7
best_mod <- 'rf'

## Run ML_iteration
random_forest_Score = ML_iteration (x = X, y = Y,
                                    n_iteration = n_iter,
                                    training_portion = portion,
                                    trCon = control,
                                    preProc = preProc,
                                    n.tune = n_tune,
                                    model = best_mod,
                                    outcome = outcome)


# Load texts
data("toy_reads")

# Generate text features
feats = generate_features( toy_reads$text, meta=toy_reads,
                           sent = TRUE,
                           clean_features = TRUE,
                           read = c("Flesch","Flesch.Kincaid", "ARI"),
                           ld=c("TTR","R","K"),
                           ignore=c("ID"),
                           verbose = TRUE )
# Preprocess the feature space to remove collinear features
# and features with near-zero variance
X_all = dplyr::select( feats,
                       -ID, -Q1, -Q2, -text, -more )
X_all = predict(caret::preProcess( X_all, method = c("nzv","corr"),
                                   uniqueCut=2, cutoff=0.95), X_all )
caret::findLinearCombos(X_all) # sanity check to make sure no redundant features

# Transform all variables as numeric variables
X_all[] <- lapply(X_all,
                  function(x) if(is.character(x)) as.numeric(as.factor(x)) else x)

# Extract outcome variables
all_Scores <- toy_reads$Q1

## Set parameters
X <- X_all
Y <- all_Scores
n_iter <- 2
n_tune <- 2
control <- caret::trainControl(method = 'cv')
preProc <- 'zv'
outcome <- 'continuous'
best_mod <- 'rf'

## Define the percentages for training portions
percentages <- c(.20, 0.40, 0.60, 0.80)

## Loop through each percentage
random_forest_Scores = ML_iterations (x = X, y = Y,
                                      n_iteration = n_iter,
                                      training_portions = percentages,
                                      trCon = control,
                                      preProc = preProc,
                                      n.tune = n_tune,
                                      model = best_mod,
                                      outcome = outcome)

}
\seealso{
* `ML_iterations`: A wrapper function for performing multiple iterations of
 `ML_iteration` with different training proportions.
 * `caret::train`: The underlying function used for model training.

* `ML_iteration`: The core function performing a single iteration of model
 training and evaluation.
 * `caret::train`: The underlying function used for model training.
}
